import os.path
import numpy as np
import random
import torchvision.transforms as transforms
import torch

from data.base_dataset import BaseDataset
from data.image_folder import make_dataset
#from scipy.io import loadmat
from util.util import vox2tensor, normalize3d




class VoxelDataset(BaseDataset):
    def initialize(self, opt):
        self.opt = opt
        self.root = opt.dataroot
        #self.dir_AB = os.path.join(opt.dataroot, opt.phase)
        self.dir_AB = r"C:\Users\ortma\Desktop\FACULTAD\PI\TENSORES PACIENTES"
        # go through directory return os.path for all images
        slice_filetype = ['.pt'] 
        self.AB_paths = sorted(make_dataset(self.dir_AB, slice_filetype))  # make_dataset te devuelve una lista con tutas de las imagenes que coincidan con las extensiones ingresadas
        assert self.opt.loadSize == self.opt.fineSize, 'No resize or cropping.'

    def rgb_to_rgb(self, mat):
        ''' Reference images are CT scans with highlighted OARs and PTVs. The
        targets are 3D voxel maps with colourized dose intensities.
        '''

        "Las imágenes de referencia son tomografías computarizadas (CT) con órganos en riesgo (OAR) y volúmenes objetivo de planificación (PTV) resaltados. Los objetivos son mapas tridimensionales de píxeles (voxels) con intensidades de dosis coloreadas."

        
        loaded_var = torch.load(r"C:\Users\ortma\Desktop\FACULTAD\PI\TENSORES PACIENTES")

        
        
        #d, w, h, nc = ct_img.shape
        #assert w == self.opt.loadSize, 'size mismatch in width'
        #assert h == self.opt.loadSize, 'size mismatch in height'
        
        # to handle aaron's weird uint format
        if dose_img.dtype == np.uint8:
            dose_img = dose_img / 256
        if ct_img.dtype == np.uint8:
            ct_img = ct_img / 256

        A = vox2tensor(ct_img).float()
        B = vox2tensor(dose_img).float()

        # ABs are 3-channel. Normalizing to 0.5 mean, 0.5 std
        A = normalize3d(A, (0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
        B = normalize3d(B, (0.5, 0.5, 0.5), (0.5, 0.5, 0.5))

        # flipping augments the dataset by flipping a bunch of the images.
        #if (not self.opt.no_flip) and random.random() < 0.5:
        #    idx = [i for i in range(A.size(3) - 1, -1, -1)]
        #    idx = torch.LongTensor(idx)
        #    A = A.index_select(3, idx)
        #    B = B.index_select(3, idx)

        # flipped the width side. Q: is it worth it to flip on height as well?
        #if (not self.opt.no_flip) and random.random() < 0.5:
        #    idx = [i for i in range(A.size(2) - 1, -1, -1)]
        #    idx = torch.LongTensor(idx)
        #    A = A.index_select(2, idx)
        #    B = B.index_select(2, idx)
       # return A, B

    def rgb_to_gray(self, ct_img, dose_val):
        ''' Reference images are CT scans with highlighted OARs and PTVs. The
        targets are dose intensity matrices.
        '''

       # d, w, h, nc = ct_img.shape
       # assert (d, w, h) == dose_val.shape, 'size mismatch between dose and ct'
        
        # to handle aaron's weird uint format
        if dose_val.dtype == np.uint8 or dose_val.dtype == np.uint16:
            dose_val= dose_val / 256
        if ct_img.dtype == np.uint8 or ct_img.dtype == np.uint16:
            ct_img = ct_img / 256

        A = vox2tensor(ct_img).float()
        A = normalize3d(A, (0.5, 0.5, 0.5), (0.5, 0.5, 0.5))

        B = torch.from_numpy(dose_val).float()
        B = B.unsqueeze(0) # se utiliza para agregar una dimensión adicional al tensor B en la posición 0
        B.sub_(0.5).div_(0.5)

        # flipping augments the dataset by flipping a bunch of the images.
     #   if (not self.opt.no_flip) and random.random() < 0.5:
      #      idx = [i for i in range(A.size(2) - 1, -1, -1)]
      #      idx = torch.LongTensor(idx)
       #     A = A.index_select(2, idx)
        #    B = B.index_select(2, idx)
        return A, B

    def __getitem__(self, index):
    #    input_nc = self.opt.input_nc
      #  output_nc = self.opt.output_nc
        input_nc = 3
        output_nc = 1
        AB_path = self.AB_paths[index]
        loaded = torch.load(AB_path)
        # Acceder a los tensores por separado
        ct_img = (loaded["imagen"]).numpy()
        dose_val = (loaded["dosis"]).numpy()
        if input_nc == 3 and output_nc == 3:
            A, B = self.rgb_to_rgb(mat)
        elif input_nc == 3 and output_nc == 1:
            A, B = self.rgb_to_gray(ct_img, dose_val)
        elif input_nc == 1 and output_nc == 1:
            raise NotImplementedError('need to work out val to val.')
        else:
            raise NotImplementedError('channels dont align.')

        return {'A': A, 'B': B, 'A_paths': AB_path, 'B_paths': AB_path}

    def __len__(self):
        return len(self.AB_paths)

    def name(self):
        return 'VoxelDataset'




